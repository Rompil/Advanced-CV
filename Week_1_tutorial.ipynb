{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 1 tutorial.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1VLTlTX9ACvd3WvOOBeNZDw-iKvqvSFBK",
      "authorship_tag": "ABX9TyPF+qn+CnO6ioGjg+GncxsE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rompil/Advanced-CV/blob/main/Week_1_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTwfJRyrSDex"
      },
      "source": [
        "!pip install opencv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohJ05EQIGVBK"
      },
      "source": [
        "# Pan shapening algorithms\n",
        "\n",
        "The task is to make  a high resolution coloured image using one high resolution grayscale image and one low resolution colured image. Also, the low ser image is roteted on an arbitrary angle.\n",
        "\n",
        "**Plan of the solution**\n",
        "* Read images\n",
        "* Restore desired image resolution and orientation with Affine transformations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATaxUIVJSO4k"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UrYXjHLBlGk"
      },
      "source": [
        "## Read files to process\n",
        "\n",
        "These files are different size and with different number of colour channels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVjLk6YVIj_X"
      },
      "source": [
        "# read files for the task\n",
        "lo_res_half_path = \"/content/sample_data/RGB_half.JPG\"\n",
        "lo_res_quat_path = \"/content/sample_data/RGB_quater.JPG\"\n",
        "hi_res_gs_path = \"/content/sample_data/GRAY.JPG\"\n",
        "lo_res_half = cv2.imread(lo_res_half_path)\n",
        "lo_res_quat = cv2.imread(lo_res_quat_path)\n",
        "hi_res_gs = cv2.imread(hi_res_gs_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5TNcmqlCBgB"
      },
      "source": [
        "## Restore desired image resolution and orientation with Affine transformations\n",
        "\n",
        "The idea behind this approach is to find a key feature set in both images and apply affine transformation to the coloured image to restore a resolution and an orientation based on these ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MijqalR8vgRS"
      },
      "source": [
        "def transform_image(original_image, transform_image):\n",
        "    \"\"\"\n",
        "    The finction transforms ~transform image~ to ~original image~ based on key feature points and affine transformations\n",
        "    \"\"\"\n",
        "    # Initiate ORB detector\n",
        "    orb = cv2.ORB_create()\n",
        "    # find the keypoints and descriptors with ORB\n",
        "    kp1, des1 = orb.detectAndCompute(transform_image,None)\n",
        "    kp2, des2 = orb.detectAndCompute(original_image,None)\n",
        "    # create BFMatcher object\n",
        "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "    # Match descriptors.\n",
        "    matches = bf.match(des1,des2)\n",
        "    # Sort them in the order of their distance.\n",
        "    matches = sorted(matches, key = lambda x:x.distance)\n",
        "    rows,cols,ch = original_image.shape\n",
        "    #Only three points are need for the Affine transformation\n",
        "    pts1 = np.array([kp1[match.queryIdx].pt for match in matches[:3]]).astype(np.float32)\n",
        "    pts2 = np.array([kp2[match.trainIdx].pt for match in matches[:3]]).astype(np.float32)\n",
        "\n",
        "    M = cv2.getAffineTransform(pts1,pts2)\n",
        "    dst = cv2.warpAffine(transform_image,M,(cols,rows)) \n",
        "    return dst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDvAtX6KBV1h"
      },
      "source": [
        "# As far as the original image won't change let's fix one of the parameter in the function\n",
        "from functools import partial\n",
        "scale_up_transformation = partial(transform_image, hi_res_gs)\n",
        "in_image = scale_up_transformation(lo_res_half)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBhku5oH-ecI"
      },
      "source": [
        "## Pan spectral restoration\n",
        "\n",
        "Pan sharpening is used to make high resolution coloured images by combining two or more images where one is a high resolution grayscale image e.g. infrared picture and one or more rgb image but with lower resolution. There are a few algorithms how to merge these images. I'll try one of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm0zOyoz-dah"
      },
      "source": [
        "class PanSharpening:\n",
        "\n",
        "\n",
        "    def __call__(self, pan_image, rgb_image, W=0.1, method=\"Brovey\"):\n",
        "        # Firstly, let's normalize two images\n",
        "\n",
        "        \n",
        "        rgb = self._scale_and_rotate(pan_image, rgb_image)\n",
        "        if pan_image.shape[2] > 1:\n",
        "            # pan_image = np.average(pan_image, axis=2)\n",
        "            pan_image = pan_image[:,:,0]\n",
        "        if method == \"Brovey\":\n",
        "            return self._brovey(pan_image, rgb)\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "    @staticmethod\n",
        "    def _scale_and_rotate(original_image, transformed_image):\n",
        "        return transform_image(original_image, transformed_image)\n",
        "\n",
        "    def _brovey(self, pan, rgb):\n",
        "        # cv2_imshow(pan)\n",
        "        # cv2_imshow(rgb)\n",
        "        R = rgb[:,:,0]\n",
        "        G = rgb[:,:,1]\n",
        "        B = rgb[:,:,2]\n",
        "\n",
        "        all_sum = np.sum(rgb, axis=2) / 3\n",
        "        print((all_in == all_sum).all())\n",
        "\n",
        "        r = np.multiply(R, pan/all_sum)[:, :, np.newaxis]\n",
        "        g = np.multiply(G, pan/all_sum)[:, :, np.newaxis]\n",
        "        b = np.multiply(B, pan/all_sum)[:, :, np.newaxis]\n",
        "        # r = (0.5 * R + 0.5 * pan)[:, :, np.newaxis]\n",
        "        # g = (0.5 * G + 0.5 * pan)[:, :, np.newaxis]\n",
        "        # b = (0.5 * B + 0.5 * pan)[:, :, np.newaxis]\n",
        "\n",
        "        image = np.concatenate([r,g,b], axis=2)\n",
        "        return image\n",
        "    pass\n",
        "\n",
        "pan_sharp = PanSharpening()\n",
        "dst = pan_sharp(hi_res_gs, lo_res_quat)\n",
        "\n",
        "cv2_imshow(dst)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBZRlbf3j46K"
      },
      "source": [
        "t= (115,200)\n",
        "print(in_image[t])\n",
        "hi_res_gs[t]\n",
        "R = in_image[:,:,0]\n",
        "G = in_image[:,:,1]\n",
        "B = in_image[:,:,2]\n",
        "all_in = np.sum(in_image, axis=2)\n",
        "all_in[t]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}